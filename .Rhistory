library(rstan)
time_index <- 1:30
time_index
unit_index <- 1:10
unit_index
# simulation for checking the distribution of correlations
#value <- NA
#for(j in 1:1000){
x <- matrix(NA, nrow=length(time_index), ncol=length(unit_index))
x[1,] <- rnorm(length(unit_index), mean=0, sd=1)
x[1,]
for(i in 2:length(time_index)){
x[i,] <- rnorm(length(unit_index), mean=x[i-1,], sd=.25)
}
x
par(mfrow=c(1,1))
plot(x[,1], type="n", ylim=c(-3,3))
for(i in 1:10){
lines(x[,i], col=i)
}
#MASS::truehist(x)
#cbind(x[2:length(time_index)], x[1:(length(time_index)-1)])
#value[j] <- cor(x[2:length(time_index)], x[1:(length(time_index)-1)])
#}
#MASS::truehist(value)
## stack the columns of x
x_column <- c(x)
length(x_column)
unit_column <- rep(unit_index, each=30)
time_column <- rep(time_index, 10)
cbind(x_column, unit_column, time_column)
alpha1 <- -2
beta1 <- 2
prob_y1 <- inv.logit(alpha1 + beta1*x_column + rnorm(length(x_column)))
y1 <- rbinom(length(x_column), 1, prob=prob_y1)
#y1 <- alpha + beta*x +
alpha2 <- 0
beta2 <- 1
prob_y2 <- inv.logit(alpha2 + beta2*x_column + rnorm(length(x_column)))
y2 <- rbinom(length(x_column), 1, prob=prob_y2)
alpha3 <- -1
beta3 <- 2
prob_y3 <- inv.logit(alpha3 + beta3*x + rnorm(length(x_column)))
y3 <- rbinom(length(x_column), 1, prob=prob_y3)
alpha4 <- 2
beta4 <- 5
prob_y4 <- inv.logit(alpha4 + beta4*x_column + rnorm(length(x_column)))
y4 <- rbinom(length(x_column), 1, prob=prob_y4)
additive_scale <- y1 + y2 + y3 + y4
plot(additive_scale ~ x_column)
y_column <- c(y1,y2,y3,y4)
item_column <- rep(1:4, each=300)
cbind(y_column, item_column)
unit_column <- rep(unit_column, 4)
time_column <- rep(time_column, 4)
cbind(y_column, item_column, unit_column, time_column)[1:100,]
cbind(y_column, item_column, unit_column, time_column)[301:400,]
unit_time_column <- rep(1:300, times=4)
length(unit_time_column)
missing_ids <- sample(1:length(y_column), size=10, replace=FALSE)
missing_ids
y_column[missing_ids] <- NA
summary(y_column)
y_column <- y_column[-missing_ids]
summary(y_column)
item_column <- item_column[-missing_ids]
unit_column <- unit_column[-missing_ids]
time_column <- time_column[-missing_ids]
unit_time_column <- unit_time_column[-missing_ids]
## define the stan model
model <- "
data{
int n_t;
int n_t_k;
int k;
int<lower=0, upper=1> y[n_t_k];
int<lower=1> item_column[n_t_k];
int<lower=1> unit_column[n_t_k];
int<lower=1> time_column[n_t_k];
int<lower=1> unit_time_column[n_t_k];
}
parameters{
real theta[n_t];
real alpha[k];
real<lower=0> beta[k];
real<lower=0> sigma;
}
transformed parameters{
real theta_star[n_t];
real sigma_star[n_t];
real theta_star_df[n_t];
real xb[n_t_k];
for(i in 1:n_t){
if(time_column[i]==1){
theta_star[i]=0;
sigma_star[i]=1;
theta_star_df[i]=1000;
}
else{
theta_star[i]=theta_star[i-1];
sigma_star[i]=sigma;
theta_star_df[i]=5;
}
}
for(i in 1:n_t_k){
xb[i] = alpha[item_column[i]] + beta[item_column[i]] * theta[unit_time_column[i]];
}
}
model{
// priors
//theta ~ normal(theta_star, sigma_star);
theta ~ student_t(theta_star_df, theta_star, sigma_star);
alpha ~ normal(0,10);
beta ~ normal(0,10);
sigma ~ normal(0,1);
// likelihood
//for(i in 1:n_t_k){
//  y[i] ~ bernoulli_logit(alpha[item_column[i]] + beta[item_column[i]] * theta[unit_time_column[i]]);
//}
y ~ bernoulli_logit(xb);
}
"
data_list <- list(n_t=300, n_t_k=length(unit_column), k=4, y=y_column, item_column=item_column, unit_column=unit_column, time_column=time_column, unit_time_column=unit_time_column)
data_list
fit <- stan(model_code=model, data=data_list, iter=2000, chains=4, pars=c("theta_star", "sigma_star", "xb"), include=FALSE)
fit
output <- extract(fit)
dim(output)
names(output)
dim(output$theta)
names(output$theta)
theta_hat <- apply(output$theta, MARGIN=2, FUN=mean)
theta_hat
par(mfrow=c(1,1))
plot(x=x, y=theta_hat, xlab="true x", ylab="estiamted theta of x")
abline(reg=lm(theta_hat~x_column),col=2)
cor(x_column, theta_hat)
cor(additive_scale, theta_hat)
apply(output$alpha, MARGIN=2, FUN=mean)
c(alpha1, alpha2, alpha3, alpha4)
apply(output$beta, MARGIN=2, FUN=mean)
c(beta1, beta2, beta3, beta4)
alpha_hat <- apply(output$alpha, MARGIN=2, FUN=mean)
beta_hat <- apply(output$beta, MARGIN=2, FUN=mean)
## inflection points in the latent space
inflection_points <- - alpha_hat / beta_hat
x_seq <- seq(-4,4,.1)
prob_y1_hat <- inv.logit(alpha_hat[1] + beta_hat[1] * x_seq)
prob_y2_hat <- inv.logit(alpha_hat[2] + beta_hat[2] * x_seq)
prob_y3_hat <- inv.logit(alpha_hat[3] + beta_hat[3] * x_seq)
prob_y4_hat <- inv.logit(alpha_hat[4] + beta_hat[4] * x_seq)
par(mfrow=c(2,2))
plot(x=x_seq, y=prob_y1_hat, type="l")
abline(v=inflection_points[1], col=2); abline(h=.5, lty=2)
plot(x=x_seq, y=prob_y2_hat, type="l")
abline(v=inflection_points[2], col=2); abline(h=.5, lty=2)
plot(x=x_seq, y=prob_y3_hat, type="l")
abline(v=inflection_points[3], col=2); abline(h=.5, lty=2)
plot(x=x_seq, y=prob_y4_hat, type="l")
abline(v=inflection_points[4], col=2); abline(h=.5, lty=2)
#apply(output$sigma, MARGIN=2, FUN=mean)
mean(output$sigma)
boxplot(theta_hat~y1)
boxplot(theta_hat~y2)
boxplot(theta_hat~y3)
boxplot(theta_hat~y4)
MASS::truehist(theta_hat)
MASS::truehist(x)
## adds missingness
##
library(boot)
library(rstan)
time_index <- 1:30
time_index
unit_index <- 1:10
unit_index
# simulation for checking the distribution of correlations
#value <- NA
#for(j in 1:1000){
x <- matrix(NA, nrow=length(time_index), ncol=length(unit_index))
x[1,] <- rnorm(length(unit_index), mean=0, sd=1)
x[1,]
for(i in 2:length(time_index)){
x[i,] <- rnorm(length(unit_index), mean=x[i-1,], sd=.25)
}
x
par(mfrow=c(1,1))
plot(x[,1], type="n", ylim=c(-3,3))
for(i in 1:10){
lines(x[,i], col=i)
}
#MASS::truehist(x)
#cbind(x[2:length(time_index)], x[1:(length(time_index)-1)])
#value[j] <- cor(x[2:length(time_index)], x[1:(length(time_index)-1)])
#}
#MASS::truehist(value)
## stack the columns of x
x_column <- c(x)
length(x_column)
unit_column <- rep(unit_index, each=30)
time_column <- rep(time_index, 10)
cbind(x_column, unit_column, time_column)
alpha1 <- -2.5
beta1 <- 2
prob_y1 <- inv.logit(alpha1 + beta1*x_column + rnorm(length(x_column)))
y1 <- rbinom(length(x_column), 1, prob=prob_y1)
#y1 <- alpha + beta*x +
alpha2 <- -.75
beta2 <- 2
prob_y2 <- inv.logit(alpha2 + beta2*x_column + rnorm(length(x_column)))
y2 <- rbinom(length(x_column), 1, prob=prob_y2)
alpha3 <- 0.75
beta3 <- 2
prob_y3 <- inv.logit(alpha3 + beta3*x + rnorm(length(x_column)))
y3 <- rbinom(length(x_column), 1, prob=prob_y3)
alpha4 <- 2.5
beta4 <- 5
prob_y4 <- inv.logit(alpha4 + beta4*x_column + rnorm(length(x_column)))
y4 <- rbinom(length(x_column), 1, prob=prob_y4)
additive_scale <- y1 + y2 + y3 + y4
plot(additive_scale ~ x_column)
y_column <- c(y1,y2,y3,y4)
item_column <- rep(1:4, each=300)
cbind(y_column, item_column)
unit_column <- rep(unit_column, 4)
time_column <- rep(time_column, 4)
cbind(y_column, item_column, unit_column, time_column)[1:100,]
cbind(y_column, item_column, unit_column, time_column)[301:400,]
unit_time_column <- rep(1:300, times=4)
length(unit_time_column)
missing_ids <- sample(1:length(y_column), size=10, replace=FALSE)
missing_ids
y_column[missing_ids] <- NA
summary(y_column)
y_column <- y_column[-missing_ids]
summary(y_column)
item_column <- item_column[-missing_ids]
unit_column <- unit_column[-missing_ids]
time_column <- time_column[-missing_ids]
unit_time_column <- unit_time_column[-missing_ids]
## define the stan model
model <- "
data{
int n_t;
int n_t_k;
int k;
int<lower=0, upper=1> y[n_t_k];
int<lower=1> item_column[n_t_k];
int<lower=1> unit_column[n_t_k];
int<lower=1> time_column[n_t_k];
int<lower=1> unit_time_column[n_t_k];
}
parameters{
real theta[n_t];
real alpha[k];
real<lower=0> beta[k];
real<lower=0> sigma;
}
transformed parameters{
real theta_star[n_t];
real sigma_star[n_t];
real theta_star_df[n_t];
real xb[n_t_k];
for(i in 1:n_t){
if(time_column[i]==1){
theta_star[i]=0;
sigma_star[i]=1;
theta_star_df[i]=1000;
}
else{
theta_star[i]=theta_star[i-1];
sigma_star[i]=sigma;
theta_star_df[i]=5;
}
}
for(i in 1:n_t_k){
xb[i] = alpha[item_column[i]] + beta[item_column[i]] * theta[unit_time_column[i]];
}
}
model{
// priors
//theta ~ normal(theta_star, sigma_star);
theta ~ student_t(theta_star_df, theta_star, sigma_star);
alpha ~ normal(0,10);
beta ~ normal(0,10);
sigma ~ normal(0,1);
// likelihood
//for(i in 1:n_t_k){
//  y[i] ~ bernoulli_logit(alpha[item_column[i]] + beta[item_column[i]] * theta[unit_time_column[i]]);
//}
y ~ bernoulli_logit(xb);
}
"
data_list <- list(n_t=300, n_t_k=length(unit_column), k=4, y=y_column, item_column=item_column, unit_column=unit_column, time_column=time_column, unit_time_column=unit_time_column)
data_list
fit <- stan(model_code=model, data=data_list, iter=2000, chains=4, pars=c("theta_star", "sigma_star", "xb"), include=FALSE)
fit
output <- extract(fit)
dim(output)
names(output)
dim(output$theta)
names(output$theta)
theta_hat <- apply(output$theta, MARGIN=2, FUN=mean)
theta_hat
par(mfrow=c(1,1))
plot(x=x, y=theta_hat, xlab="true x", ylab="estiamted theta of x")
abline(reg=lm(theta_hat~x_column),col=2)
cor(x_column, theta_hat)
cor(additive_scale, theta_hat)
apply(output$alpha, MARGIN=2, FUN=mean)
c(alpha1, alpha2, alpha3, alpha4)
apply(output$beta, MARGIN=2, FUN=mean)
c(beta1, beta2, beta3, beta4)
alpha_hat <- apply(output$alpha, MARGIN=2, FUN=mean)
beta_hat <- apply(output$beta, MARGIN=2, FUN=mean)
## inflection points in the latent space
inflection_points <- - alpha_hat / beta_hat
x_seq <- seq(-4,4,.1)
prob_y1_hat <- inv.logit(alpha_hat[1] + beta_hat[1] * x_seq)
prob_y2_hat <- inv.logit(alpha_hat[2] + beta_hat[2] * x_seq)
prob_y3_hat <- inv.logit(alpha_hat[3] + beta_hat[3] * x_seq)
prob_y4_hat <- inv.logit(alpha_hat[4] + beta_hat[4] * x_seq)
par(mfrow=c(2,2))
plot(x=x_seq, y=prob_y1_hat, type="l")
abline(v=inflection_points[1], col=2); abline(h=.5, lty=2)
plot(x=x_seq, y=prob_y2_hat, type="l")
abline(v=inflection_points[2], col=2); abline(h=.5, lty=2)
plot(x=x_seq, y=prob_y3_hat, type="l")
abline(v=inflection_points[3], col=2); abline(h=.5, lty=2)
plot(x=x_seq, y=prob_y4_hat, type="l")
abline(v=inflection_points[4], col=2); abline(h=.5, lty=2)
#apply(output$sigma, MARGIN=2, FUN=mean)
mean(output$sigma)
boxplot(theta_hat~y1)
boxplot(theta_hat~y2)
boxplot(theta_hat~y3)
boxplot(theta_hat~y4)
MASS::truehist(theta_hat)
MASS::truehist(x)
boxplot(theta_hat~additive_scale)
par(mfrow=c(1,1))
boxplot(theta_hat~additive_scale)
## R_Demo_Measurement_factor_analysis.R file
##########################################################################
## INSTRUCTOR: Christopher Fariss
## COURSE NAME: Advanced Computational Methods for Social Media and Textual Data (3B)
## University of Essex Summer School 2023
##
## Date: 2023-08-11
##
## Please e-mail me if you find any errors or have and suggestions (either email is fine)
## e-mail: cjf0006@gmail.com
## e-mail: cjfariss@umich.edu
##########################################################################
## Introduction to tutorial:
##
## For a factor a analysis model, there is a latent trait theta_i.
##
## The subscript i = 1,... ,N indicates multiple units. y_ik is the observed value for item k for unit i.
##
## For each observed item alpha_k and \beta_k are also estimated.
##
## alpha_k continues to act as "difficulty" parameters, or threshold that benchmarks how likely an indicator is to be observed relative to the values of the latent trait.
##
## In this formulation, this is analogous to an intercept in a traditional linear regression model.
##
## beta_k, often referred to as the "discrimination" parameters and is the analogue of a slope coefficient in a linear regression.
##
##########################################################################
rm(list=ls())
## load library
library(MASS) # load library with truehist function
n <- 100
theta <- rnorm(n,0,1)
## set parameters for each item
## alpha (the intercept) is the difficulty parameter or base-line probability of 1
## beta (the slope) is the discrimination parameter or the strength of the relationship
## between the estimated latent trait theta and the individual item
## item 1 is the most difficult and least informative item
alpha1 <- -1.000000
beta1 <- 1.000000
## item 2 is of medium difficulty relative to the two other items
## it is also the same level of informativeness as item1
alpha2 <- 0.000000
beta2 <- 1.000000
## item 3 is the least difficult item but is also the most informative
## these statements are each relative to the other items in the model
alpha3 <- 1.000000
beta3 <- 3.000000
## define k as the number of items
k <- 3
## linear terms of the item specific models f() that link the latent trait to the items
x1 <- alpha1 + beta1 * theta + rnorm(n)
x2 <- alpha2 + beta2 * theta + rnorm(n)
x3 <- alpha3 + beta3 * theta + rnorm(n)
## create matrix of observed items
x <- cbind(x1, x2, x3)
## fit linear factor model
fit <- factanal(x, factor=1, scores="regression")
fit
MASS::truehist(fit$scores)
## what are the loadings?
cor(fit$scores, scale(x1))
cor(fit$scores, scale(x2))
cor(fit$scores, scale(x3))
?factanal
## uniquenesses calculation
sqrt(mean((fit$scores - scale(x1))^2))
## uniquenesses calculation
sumary(fit$scores - scale(x1))
## uniquenesses calculation
summary(fit$scores - scale(x1))
## plot true latent variable with posterior mean
par(mar=c(4,4,1,1), font=2, font.lab=2, cex=1.3)
plot(fit$scores, theta, xlim=c(-3,3), ylim=c(-3,3), ylab="true theta", xlab="factor analysis scores")
abline(a=0, b=1, col=2, lwd=2)
D <- nrow(x)
parameters_list <- c(c(1,1,1), runif(D,-1,1))
parameters_list <- c(c(1,1,1), scale(apply(x,1,mean))[,1])
# user defined function passed to optim
factor_analysis_func <- function(par, y, iterate=TRUE){
b <- par[1:3]
theta <- par[4:length(par)]
theta <- (theta - mean(theta))/sd(theta)
y.hat1 <- b[1]*theta
y.hat2 <- b[2]*theta
y.hat3 <- b[3]*theta
#y.hat1 <- (y.hat1-mean(y.hat1))/sd(y.hat1)
#y.hat2 <- (y.hat2-mean(y.hat2))/sd(y.hat2)
#y.hat3 <- (y.hat3-mean(y.hat3))/sd(y.hat3)
y[,1] <- (y[,1] - mean(y[,1]))/sd(y[,1])
y[,2] <- (y[,2] - mean(y[,2]))/sd(y[,2])
y[,3] <- (y[,3] - mean(y[,3]))/sd(y[,3])
## sse
#out <- -sum((y[,1]-y.hat1)^2 + (y[,2]-y.hat2)^2 + (y[,3]-y.hat3)^2)
## mle
out <- sum(log(dnorm(y[,1], mean=y.hat1, sd=1)) + log(dnorm(y[,2], mean=y.hat2, sd=1)) + log(dnorm(y[,3], mean=y.hat3, sd=1)))
return(out)
}
out <- optim(par = parameters_list, fn=factor_analysis_func, y=x, method="BFGS", control=list(fnscale = -1), hessian = TRUE)
out
names(out)
out_scores <- out$par[4:length(out$par)]
out$par[1:3]
## plot true latent variable with posterior mean
par(mar=c(4,4,1,1), font=2, font.lab=2, cex=1.3)
plot(out_scores, theta, xlim=c(-3,3), ylim=c(-3,3), ylab="true theta", xlab="factor analysis scores")
abline(a=0, b=1, col=2, lwd=2)
cor(fit$scores, theta) ## base factanal output and the true scores
cor(out_scores, theta) ## my function and the true scores
cor(fit$scores, out_scores) ## base factanal output and my function
plot(fit$scores, out_scores)
cov(x)
cor(x)
solve(cov(x))
loadings <- c(1,1,1)
t(loadings) %*% solve(cov(x)) %*% t(x)
D <- nrow(x)
parameters_list <- c(1,1,1)
# user defined function passed to optim
factor_analysis_func_v2 <- function(par, y, iterate=TRUE){
#a <- par[1:3]
#b <- par[4:6]
#theta <- par[7:length(par)]
y[,1] <- (y[,1] - mean(y[,1]))/sd(y[,1])
y[,2] <- (y[,2] - mean(y[,2]))/sd(y[,2])
y[,3] <- (y[,3] - mean(y[,3]))/sd(y[,3])
loadings <- par[1:3]
theta_hat <<- t(loadings) %*% solve(cov(y)) %*% t(y)
theta_hat <<- (theta_hat - mean(theta_hat))/sd(theta_hat)
y.hat1 <- loadings[1] * theta_hat
y.hat2 <- loadings[2] * theta_hat
y.hat3 <- loadings[3] * theta_hat
#y.hat1 <- (y.hat1-mean(y.hat1))/sd(y.hat1)
#y.hat2 <- (y.hat2-mean(y.hat2))/sd(y.hat2)
#y.hat3 <- (y.hat3-mean(y.hat3))/sd(y.hat3)
## sse
#out <- -sum((y[,1]-y.hat1)^2 + (y[,2]-y.hat2)^2 + (y[,3]-y.hat3)^2)
## mle
out <- sum(log(dnorm(y[,1], mean=y.hat1, sd=1)) + log(dnorm(y[,2], mean=y.hat2, sd=1)) + log(dnorm(y[,3], mean=y.hat3, sd=1)))
return(out)
}
## optim methods: "Nelder-Mead", "BFGS", "CG", "L-BFGS-B", "SANN",
out <- optim(par = parameters_list, fn=factor_analysis_func_v2, y=x, method="BFGS", control=list(fnscale = -1), hessian = TRUE)
out
names(out)
out$par
theta_hat
## this doesn't work because you need to standardize x
plot(t(out$par) %*% solve(cov(x)) %*% t(x), theta_hat)
plot(t(out$par) %*% solve(cov(x)) %*% t(x), theta)
## this works because we are standardizing x
x_standardized <- scale(x)[,1:3]
plot(t(out$par) %*% solve(cov(x_standardized)) %*% t(x_standardized), theta)
abline(a=0, b=1, col=2, lwd=2)
cor(c(t(out$par) %*% solve(cov(x_standardized)) %*% t(x_standardized)), theta)
## these values should be exactly the same up to some rounding errors (theta_hat was made by the second function)
plot(t(out$par) %*% solve(cov(x_standardized)) %*% t(x_standardized), theta_hat)
cor(c(t(out$par) %*% solve(cov(x_standardized)) %*% t(x_standardized)), c(theta_hat))
cor(fit$scores, theta) ## base factanal output and the true scores
cor(out_scores, theta) ## my function and the true scores
cor(fit$scores, out_scores) ## base factanal output and my function
fit
## uniquenesses calculation
sum((fit$scores - scale(x1))^2)
sum((fit$scores - scale(x2))^2)
sum((fit$scores - scale(x3))^2)
attributs(fit)
attributes(fit)
fit$uniquenesses
fit$correlation
factanal
